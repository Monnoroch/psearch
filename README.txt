Инструкция по применению.
Все пока в супер-альфа стадии, и ничего автономно еще не работает.

Сервисы:

I. Резолвер DNS /crawler/dns/bin

II. Загрузчик документов /crawler/downloader/bin

III. Балансеры. /balanser/http/bin и /balanser/tcp/bin.

IV. Хранилище данных /gatekeeper/bin.
В него можно писать, искать и читать большие значения по маленьким ключам.
Работает он так:
- Загрузка состояния с диска (см в конце)
- Запись (key, v) -- дописывание пары в текущий чанк.
- Запись (key, чанк, оффсет, длина) в трай в памяти по ключу.
- Текущий чанк периодически синкаем.
- Когда текущий чанк переполнился, сохраняем его и переходим к следующему (пока пустому). TODO: Устроить в этом месте репликацию.

При падении снова происходит загрузка:
- Перебираем все чанки и все пары (key, v) в каждом, пишем в трай (key, чанк, оффсет, длина).
- Вуаля, трай пересобран, можно работать!
- Удаляем чанки, в которых не осталось актуальных данных (не перезаписаных поздними чанками).

TODO: мастер-слейв архитекрура, репликация на слейвы законченных чанков.
Еще TODO: сделать мержер чанков, который в бэкграунде будет брать старые чанки и мержить.

Идея в том, что только законченные чанки будут сохранены, непосинканные данные из незаконченных могут пропасть при падении мастера.
Ну и хрен с ними, это же хранилище для краулера, перекачаем!

V. Менеджер загрузок. /crawler/caregiver/bin
Эта штука должна принимать запросы на загрузку урлов и асинхронно отдавать результаты.
При этом, она еще должна не нагружать сильно отдельных хосты, и в будущем планируется резолв и
кеширование dns с помощью /crawler/dns/bin (пока не работает из за того, что по IP мне дают не то, что по хосту, WAT?!).

===========

Json-rpc клиент для тестирования лежит в /test/rpcclient

Пишешь:

go run main.go -addr АДРЕС_СЕРВИСА -method DownloaderServer.Download -arg '{"url": "http://habrahabr.ru"}'

Получаешь html от /crawler/downloader/bin. Профит.
